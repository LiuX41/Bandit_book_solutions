%!TEX root =  main.tex

\chapter*{Chapter 3 Stochastic Processes and Markov Chains}
\label{sec:second}

\noindent\textbf{3.1} Fill in the details of Theorem 3.1:


\begin{enumerate}
   \item[(a)] Prove that $F_t \in \set{0,1}$ is a Bernoulli random variable for all $t \ge 1$.
   \item[(b)] In what follows, equip $\cS$ with $\mathbb{P}=\lambda$, the uniform probability measure. Show that for any $t \ge 1$, $F_t$ is uniformly distributed: $\PP{F_t= 1 } = \PP{F_t = 0} = 1/2$. 
   \item[(c)] Show that $(F_t)_{t=1}^{\infty}$ are independent.
   \item[(d)] Show that $(X_{m,t})_{t=1}^\infty$ is an independent sequence of Bernoulli random variables that are uniformly distributed.
   \item[(e)] Show that $X_t = \sum_{t=1}^\infty X_{m,t}2^{-t}$ is uniformly distributed on $[0,1]$
   \item[(f)] Show that $(X_t)_{t=1}^\infty$ are independent.
\end{enumerate}

\begin{proof}
    \begin{enumerate}
   \item[(a)] It can be shown that $F_t$ has the following form: 
   $$F_t(x) = \bOne{x\in U_t}, U_t = \set{1} \cup \bigcup_{0 \le s\le 2^{t-1}}\left[\frac{2s-1}{2^t}, \frac{2s}{2^t}\right)$$. \\
   Since $U_t \in \cB([0,1])$, thus we can show $F_t$ is $\cB([0,1])$-mearuable. And $F_t \in \set{0,1}$, thus it is Bernoulli random variable. 
   \item[(b)] $\PP{F_t = 1} = \PP{U_t} = \sum_{s=0}^{2^{t-1}}\frac{1}{2^t} =\frac{2^{t-1}}{2^t} =\frac{1}{2}$. And $\PP{F_t = 0} = 1-\PP{F_t = 1}=\frac{1}{2}$.
   \item[(c)] 
   \item[(d)] 
   \item[(e)] 
   \item[(f)] 
\end{enumerate}
\end{proof}




% \begin{enumerate}[(a)]
%     \item On$([0,1],\mathcal{B},\lambda)$,for any $x\in[0,1]$
    
%     Let $F_1(x),F_2(x),F_3(x)$,...be the binary expansion of x.
    
%     $F_t(x)$=
%     $\begin{cases}
%     1 , A\\
%     0 , \overline{A}\quad(\overline{A}\text{ is the opposite case of }A)
%     \end{cases}$
    
%     $F_t(x)$ is Bernoulli random variable.
    
%     \item $\begin{cases}
%     F_1=0:0\le x<0.5\\
%     F_1=1:0.5\le x<1
%     \end{cases}$
    
%     $\begin{cases}
%     F_2=0:0\le x'<0.5\\
%     F_2=1:0.5\le x'<1
%     \end{cases}$
%     \label (x'=2x-1
    
%     ...
    
%     $\begin{cases}
%     F_t=0:0\le x^t<0.5 \Rightarrow \mathbb{P}(F_t=0)=\frac{1}{2}\\
%     F_t=1:0.5\le x^t<1 \Rightarrow \mathbb{P}(F_t=1)=\frac{1}{2}
%     \end{cases}$
    
%     \item It is obviously that $(F_t)^\infty_{t=1}$ are independent. It satisfies independent equation: $\mathbb{P}(A\cap B)=\mathbb{P}(A)\mathbb{P}(B)$.
    
%     \item $(X_{m,t})^\infty_{t=1}$ is a subsequence of $(F_t)^\infty_{t=1}$ and $(X_{m,t})^\infty_{t=1}$ are mutually exclusive.
%     \item Such as(d).
%     \item Such as(d).
%     \end{enumerate}

\noindent\textbf{3.2}
(\textsc{Martingales and optional stopping}) Let $\left(X_{t}\right)_{t=1}^{\infty}$ be an infinite sequence of independent
Rademacher random variables and $S_{t}=\sum_{s=1}^{t} X_{s} 2^{s-1}$.
\begin{enumerate}
    \item[(a)] Show that $\left(S_{t}\right)_{t=0}^{\infty}$ is a martingale.
    \item[(b)] Let $\tau=\min \left\{t: S_{t}=1\right\}$ and show that $\mathbb{P}(\tau<\infty)=1$. 
    \item[(c)] What is $\mathbb{E}\left[S_{\tau}\right]$?
    \item[(d)] Explain why this does not contradict Doob's optional stopping theorem. 
\end{enumerate}

\begin{proof}
    \begin{enumerate}
        \item[(a)] First of all, we can observe that $S_t$ is $\mathcal{F}_{t}$-measurable given that it is actually a function of random variables $X_1, \cdots, X_t$ on $(\Omega, \mathcal{F}, \mathbb{P})$.
        
        Next, we have
        \begin{equation*}
            \begin{aligned}
                \mathbb{E}\left[S_{t} \mid \mathcal{F}_{t-1}\right] 
                &= \mathbb{E}\left[S_{t-1} + X_t 2^{t-1}\mid \mathcal{F}_{t-1}\right]\\
                &= \mathbb{E}\left[S_{t-1} \mid \mathcal{F}_{t-1}\right] + \mathbb{E}\left[X_t 2^{t-1} \mid \mathcal{F}_{t-1}\right]\\
                &= S_{t-1} + \mathbb{E}\left[X_t 2^{t-1} \mid \mathcal{F}_{t-1}\right]\\
                &= S_{t-1} + \frac{1}{2} 2^{t-1} + \frac{1}{2} (-2^{t-1})\\
                &= S_{t-1}.
            \end{aligned}
        \end{equation*}

        Finally, as a weighted sum of finite random varialbes, $S_t$ is surely integrable.

        \item[(b)] Notice that 
        \begin{equation*}
            \begin{aligned}
                \mathbb{P}(\tau = n) 
                &= \mathbb{P}(\tau \neq 1) \mathbb{P}(\tau \neq 2 \mid \tau \neq 1) \cdots \mathbb{P}(\tau = n \mid \tau \notin \{1, \cdots n - 1\})\\
                &= \mathbb{P}(X_1 = -1) \mathbb{P}(X_2 = -1 \mid X_1 = -1) \cdots \mathbb{P}(X_n = 1 \mid X_1 = \cdots = X_{n-1} = -1)\\
                &= \frac{1}{2^n},
            \end{aligned}
        \end{equation*}
        which implies $\mathbb{P}(\tau = \infty) = \lim_{n \rightarrow \infty} \mathbb{P}(\tau = n) = 0$ and hence $\mathbb{P}(\tau < \infty) = 1 - \mathbb{P}(\tau = \infty) = 1$.

        \item[(c)] It is obvious that $\mathbb{E}(S_\tau) = 1$ according to our stopping rule.
        \item[(d)] The result of (c) does not contradict Doob's optional stopping theorem because the conditions are not satisfied.
        Firstly, $\mathbb{P}(\tau > n) = \frac{1}{2^n} > 0$ for any $n \in \mathbb{N}$, which breaks the first condition.
        Next, $|S_{t+1} - S_{t}| = 2^t$ and $|S_{t \wedge \tau}| = |\sum_{s=1}^{t \wedge \tau} X_s 2^{s-1}$ can neither not be bounded, which breaks the second and third condition.
    \end{enumerate}
\end{proof}

\noindent\textbf{3.3}
(\textsc{Martingales and optional stopping (ii)}) Give an example of a martingale $\left(S_{n}\right)_{n=0}^{\infty}$ and
stopping time $\tau$ such that $$\lim _{n \rightarrow \infty} \mathbb{E}\left[S_{\tau \wedge n}\right] \neq \mathbb{E}\left[S_{\tau}\right].$$
\begin{proof}
    Notice that $(S_{\tau \wedge n})_n^\infty$ is also a martingale (which is usually called stopped martingale).
    This can be shown by formulating $S_{\tau \wedge n} = S_{\tau \wedge (n-1)} + \mathbb{I}\left\{\tau \leq n\right\} (S_n - S_{n-1})$ and checking
    \begin{equation*}
        \begin{aligned}
            \mathbb{E}[S_{\tau \wedge n} | \mathcal{F}_{n-1}]
            &= \mathbb{E}[S_{\tau \wedge (n-1)} + \mathbb{I}\left\{\tau \leq n\right\} (S_n - S_{n-1}) | \mathcal{F}_{n-1}]\\
            &= \mathbb{E}[S_{\tau \wedge (n-1)} | \mathcal{F}_{n-1}] + \mathbb{E}[\mathbb{I}\left\{\tau \leq n\right\} (S_n - S_{n-1}) | \mathcal{F}_{n-1}]\\
            &= S_{\tau \wedge (n-1)} + \mathbb{I}\left\{\tau \leq n\right\}(S_{n-1} - S_{n-1})\\
            &= S_{\tau \wedge (n-1)}.
        \end{aligned}
    \end{equation*}

    Therefore, we have $\mathbb{E}(S_{\tau \wedge n}) = \mathbb{E}(S_{\tau \wedge 0}) = \mathbb{E}(S_{0})$.
    Now it suffices to give the same example as in 3.2 that $\lim _{n \rightarrow \infty} \mathbb{E}\left[S_{\tau \wedge n}\right] = \mathbb{E}(S_{0}) \neq \mathbb{E}\left[S_{\tau}\right]$.
\end{proof}


\noindent\textbf{3.4} If $X_t\ge0$ is dropped, $\mathbb{E}[X_\tau|\{\tau\le n\}] \ge \mathbb{E}[\varepsilon|\{\tau\le n\}]$ not always true.




\noindent\textbf{3.5}
Let $(\Omega, \mathcal{F})$ and $(\chi,\mathcal{G})$ be measurable spaces, $X: \chi \to \mathbb{R}$ be a random variable and $K: \Omega\times \mathcal{G} \to [0,1]$ a prbability kernel from $(\Omega, \mathcal{F})$ to $(\chi,\mathcal{G})$. Define function $U:\Omega \to \mathbb{R}$ by $U(\omega) = \int_\chi X(x) K(\omega,dx)$ and assume that $U(\omega)$ exists for all $\omega$. Prove that $U$ is measurable. 
\begin{proof}
    When $X(x)$ is simple function, $X(x) = \sum_{i = 1}^n \alpha_i I\{A_i\}$,
    \begin{equation}
        \begin{aligned}
            U(\omega) &= \int_\chi \sum_{i = 1}^n \alpha_i I\{A_i\} K(w,dx)\\
            &= \sum_{i = 1}^n \alpha_i K(w,A_i\bigcap\chi)\ \text{is measuable}
        \end{aligned}
    \end{equation}
    When $X(x)$ is non-negative function, $X(x) = \sup \{h: h \text{ is simple and } 0\leq h \leq X\}$, 
    \begin{equation}
        \begin{aligned}
            U(\omega) &= \int_\chi \sup h(x) K(w,dx)\\
            & = \sup \int_\chi h(x) K(w,dx) \ \text{is measurable (due to dominated convergence thm)} 
        \end{aligned}
    \end{equation}
    When $X(x)$ is general function, $X = X^+ - X^-$, use the linearity of integration, we can still prove that U is measurable. 
\end{proof}



\noindent\textbf{3.6}(Limits of increasing stopping times are stopping times) Let $(\tau_n)_{\infty}$ be an almost surely increasing sequence of $\FF$-stopping times on probability space $(\Omega,\cF,\mathbb{P})$ with filtration $\FF = (\cF_n)_{n=1}^{\infty}$, which means that $\tau_n(\omega) \le \tau_{n+1}(\omega)$ for all $n\ge 1$ almost surely. Prove that $\tau(\omega)=\lim_{n\to \infty}\tau_n(\omega)$ is a $\FF$-stopping time. 

\begin{proof}
    To show $\tau$ is a stopping time, we want to show that $\forall t, \bOne{\tau \le t} \in \cF_t$. 
    \begin{align*}
        \bOne{\tau \le t} = \bOne{\lim_{n\to \infty}\tau_n \le t} = \bOne{\sup_n \tau_n \le t}   = \cap_n \bOne{\tau_n \le t} 
    \end{align*}
    Since $\forall n, \tau_n$ is a stopping time, then $\bOne{\tau_n\le t} \in \cF_t,\forall t$. And $\forall t, \cF_t$ is a $\sigma$-algebra. Thus $\forall t, \bOne{\tau \le t} = \cap_n \bOne{\tau_n \le t}  \in \cF_t$. 
\end{proof}


\noindent\textbf{3.7} (Properties of stopping times) Let $\FF = (\cF_t)_{t \in \NN}$ be a filtration, and $\tau,\tau_1,\tau_2$ be stopping times with respect to $\FF$. Show the following:


\begin{enumerate}
   \item[(a)] $\cF_\tau$ is a $\sigma$-algebra
   \item[(b)] If $\tau =k$ for some $k\ge 1$, then $\cF_\tau = \cF_k$.
   \item[(c)] If $\tau_1 \le \tau_2$, then $\cF_{\tau_1} \subseteq \cF_{\tau_2}$.
   \item[(d)] $\tau$ is $\cF_\tau$-measurable.
   \item[(e)] If $(X_t)$ is $\FF$-adapted, then $X_\tau$ is $\cF_\tau$-measurable.
   \item[(f)] $\cF_\tau$ is the smallest $\sigma$-algebra such that all $\FF$-adapted sequences $(X_t)$ satisfy $X_\tau$ is $\cF_\tau$-measurable.
\end{enumerate}