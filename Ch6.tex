%!TEX root =  main.tex
\chapter*{Chapter 6 The Explore-Then-Commit Algorithm}
\label{sec:6}

\noindent\textbf{6.1} (\textsc{Subgaussian empirical estimates})
Let $\pi$ be the policy of ETC and $P_{1}, \ldots, P_{k}$ be the 1-subgaussian distributions associated with the $k$ arms.
Provide a fully rigourous proof of the claim that
    $$\hat{\mu}_{i}(m k)-\mu_{i}-\hat{\mu}_{1}(m k)+\mu_{1}$$
is $\sqrt{2 / m}$-subgaussian.
You should only use the definitions and the interaction protocol, which states that
\begin{enumerate}
    \item $\mathbb{P}\left(A_{t} \in \cdot \mid A_{1}, X_{1}, \ldots, A_{t-1}, X_{t-1}\right)=\pi\left(\cdot \mid A_{1}, X_{1}, \ldots, A_{t-1}, X_{t-1}\right)$ a.s.
    \item $\mathbb{P}\left(X_{t} \in \cdot \mid A_{1}, X_{1}, \ldots, A_{t-1}, X_{t-1}, A_{t}\right)=P_{A_{t}}(\cdot)$ a.s.
\end{enumerate}

\begin{proof}
    By Lemma 5.4, it holds that $(\hat{\mu}_{i}(m k)-\mu_{i})$ and $(\hat{\mu}_{1}(m k)-\mu_{1})$ are both $\sqrt{1 / m}$-subgaussian.
    Hence $(\hat{\mu}_{i}(m k)-\mu_{i}) - (\hat{\mu}_{1}(m k)-\mu_{1})$ is $\sqrt{2 / m}$-subgaussian, again according to Lemma 5.4
\end{proof}

\noindent\textbf{6.2} (\textsc{Minimax regret})
Show that Eq. (6.6) implies the regret of an optimally tuned ETC for subgaus-
sian two-armed bandits satisfies $R_{n} \leq \Delta+C \sqrt{n}$ where $C>0$ is a universal constant.

\begin{proof}
    We proceed by comparing the values of $n\Delta$ and $\Delta+\frac{4}{\Delta}\left(1+\max \left\{0, \log \left(\frac{n \Delta^{2}}{4}\right)\right\}\right)$.

\begin{enumerate}
    \item[(a)] If $n\Delta > \Delta+\frac{4}{\Delta}\left(1+\max \left\{0, \log \left(\frac{n \Delta^{2}}{4}\right)\right\}\right)$,
        we have $(n - 1) \Delta^2 > 4 (1 + \max \left\{0, \log \left(\frac{n \Delta^{2}}{4}\right)\right\}) \geq 4$,
        which suggests that $\Delta \geq \frac{2}{\sqrt{n}}$.
        Therefore, \begin{equation*}
            \begin{aligned}
                R_n
                &=\Delta+\frac{4}{\Delta}\left(1+\max \left\{0, \log \left(\frac{n \Delta^{2}}{4}\right)\right\}\right)\\
                &= \Delta+\frac{4}{\Delta}\left(1+\log \left(\frac{n \Delta^{2}}{4}\right)\right)\\
                &= \Delta+\frac{4}{\Delta}+\frac{4}{\Delta}\log(\frac{n\Delta^2}{4})\\
                &\leq \Delta+2\sqrt{n}+\frac{16}{e^4}\sqrt{n}\\
                &=\Delta + C\sqrt{n},
            \end{aligned}
        \end{equation*}
        where the inequality follows from taking $x^* = \frac{2e^4}{\sqrt{n}}$ to maximize $f(x) = \frac{4}{x} log(\frac{nx^2}{4})$.

        \item[(a)] If $n\Delta \leq \Delta+\frac{4}{\Delta}\left(1+\max \left\{0, \log \left(\frac{n \Delta^{2}}{4}\right)\right\}\right)$,
        we consider another two cases:

        \begin{enumerate}
            \item[(i)] If $\Delta \geq \frac{2}{\sqrt{n}}$, by (1) we still have $R_n = n\Delta \leq \Delta+\frac{4}{\Delta}\left(1+\max \left\{0, \log \left(\frac{n \Delta^{2}}{4}\right)\right\}\right) \leq \Delta + C\sqrt{n}$.
            \item[(ii)] If $\Delta < \frac{2}{\sqrt{n}}$, $R_n \leq n\Delta \leq 2\sqrt{n} \leq \Delta + C\sqrt{n}$, where the first inequality is trivial.
        \end{enumerate}
    \end{enumerate}
\end{proof}

\noindent\textbf{6.3}
Suppose $\Delta_1 = 0$, $\Delta_2 = \Delta > 0$.
Then, the probability that we choose the suboptimal arm (i.e., the second arm) after commitment is
\begin{equation*}
    \begin{aligned}
        \mathbb{P}(T_{2}(n)>m)
        &= \mathbb{P}(\hat{\mu}_{2}(2 m) > \hat{\mu}_{1}(2 m))\\
        &= \mathbb{P}(\left[\hat{\mu}_{2}(2 m) - \mu_2\right] - \left[\hat{\mu}_{1}(2 m) - \mu_1\right]> \Delta\\
        &\leq \exp(-\frac{m\Delta^2}{4}),
    \end{aligned}
\end{equation*}
where the inequality follows from Theorem 5.3.
By letting $\exp(-\frac{m\Delta^2}{4}) = \delta$, we have $m = -\frac{4\log\delta}{\Delta^2}$.
Hence, if we take $m = \min\{\lfloor \frac{n}{2}\rfloor, -\frac{4\log\delta}{\Delta^2}\}$, with high probability we have
\begin{equation*}
    \begin{aligned}
        \bar{R}_{n}
        &= \Delta T_2(n)\\
        &\leq \Delta m\\
        &= \min\{\lfloor \frac{n}{2}\rfloor \Delta, -\frac{4\log\delta}{\Delta}\}
    \end{aligned}
\end{equation*}

\noindent\textbf{6.4} (\textsc{High-probability bounds (ii)}) Repeat the previous exercise, but now
prove a high probability bound on the random regret: $\hat{R}_n=n\mu^{\ast}-\sum^n_{t=1}X_t$.
Compare this to the bound derived for the pseudo-regret in the previous exercise.
What can you conclude?
\begin{proof}
    Denote the reward received in the $t$-th interaction with arm $i$ as $X_{i, t}$.
    From Ex. 6.3, we have that with probability $1 - \delta$,
    \begin{equation*}
        \begin{aligned}
            \hat{R}_{n}
            &\leq \sum_{t = 1}^{n - m} (\mu_1 - X_{1, t}) + \sum_{t = 1}^{m} (\mu_1 - X_{2, t})\\
            &= \sum_{t = 1}^{n - m} (\mu_1 - X_{1, t}) + \sum_{t = 1}^{m} (\mu_2 - X_{2, t}) + m\Delta\,.
        \end{aligned}
    \end{equation*}
    
    Notice that the sum of the first two terms is $(\sqrt{(n - m)^2 + m^2})$-subgaussian.
    Therefore, with probability $(1 - \delta)^2$, we have $\hat{R}_{n} \leq \sqrt{-2[(n - m)^2 + m^2] \log \delta} + m \Delta$.
    This suggests that compared to that derived for the pseudo-regret, the bound on the random regret is less tight with a smaller probability as more randomness is considered.    
\end{proof}

\noindent\textbf{6.5}
Suppose $\Delta_1 = 0$, $\Delta_2 = \Delta > 0$.

\begin{enumerate}
    \item[(a)] By Theorem 6.1, we have
    \begin{equation*}
        \begin{aligned}
            R_n(v)
            &= \Delta \mathcal{E}[T_2(n)]\\
            &\leq m\Delta + (n - 2m)\Delta \exp (-\frac{m\Delta^2}{4})\\
            &\leq m\Delta + n\Delta \exp (-\frac{m\Delta^2}{4})\\
            &\leq m\Delta + n\sqrt{\frac{2}{m}}\exp(-\frac{1}{2})\\
            &=[\Delta + \sqrt{2}\exp(-\frac{1}{2})]n^{\frac{2}{3}},
        \end{aligned}
    \end{equation*}
    where the last inequality follows from taking $x^* = \sqrt{\frac{2}{m}}$ to maximize $f(x) = x\exp(-\frac{m\Delta^2}{4})$,
    and the last equality follows from taking $m = n^{\frac{2}{3}}$.

    Assume there is such a $C > 0$ that leads to $R_{n}(v) \leq \Delta_{v}+C n^{2 / 3}$ for any problem instance $v$ and $n \geq 1$.
    Since trivially $R_n(v) \geq m\Delta$, we have $m\Delta \leq \Delta + Cn^{\frac{2}{3}} \Rightarrow m \leq 1 + \frac{Cn^{\frac{2}{3}}}{\Delta}$.
    Under this circumstance, we can easily find a problem instance $v$ with $\Delta \rightarrow \infty$ such that $m \leq 1$.
    Recalling we only explore $2m$ rounds, we will eventually pull the suboptimal arm with a high probability, which contradicts our assumption.

    \item[(c)] We proceed by comparing the values of $\frac{C \log n}{\Delta}$ and $C \sqrt{n \log (n)}$.
    \begin{enumerate}
        \item[(i)] If $\Delta \geq \sqrt{\frac{\log n}{n}}$, $R_n(v) \leq \Delta + C\frac{\log n}{\Delta} \leq \Delta + C\sqrt{n\log n}$.
        \item[(ii)] If $\Delta < \sqrt{\frac{\log n}{n}}$, $R_n(v) \leq n \Delta \leq \sqrt{n \log n} \leq \Delta + C\sqrt{n\log n}$.
    \end{enumerate}

    \item[(e)] We proceed by comparing the values of $e$ and $n \Delta^2$.
    \begin{enumerate}
        \item[(i)] If $\Delta \geq \sqrt{\frac{e}{n}}$, $R_n(v) \leq \Delta + \frac{C\log(n\Delta^2)}{\Delta} \leq \Delta + \frac{2C}{e}\sqrt{n} = \Delta + C\sqrt{n}$.
        \item[(ii)] If $\Delta < \sqrt{\frac{e}{n}}$, $R_n(v) \leq n \Delta \leq \sqrt{en} \leq \Delta + C\sqrt{n}$.
    \end{enumerate}
\end{enumerate}

\textbf{\noindent\textbf{6.6}}

The purpose of this exercise is to analyse a meta-algorithm based on the so-called \textbf{doubling trick} that converts a policy depending on the horizon to a policy with similar
guarantees that does not. Let E be an arbitrary set of bandits. Suppose you are given a policy $\pi=\pi(n)$
designed for E that accepts the horizon n as a parameter and has a regret guarantee of
\begin{align*}
    \max_{1\leq t\leq n} R_t(\pi(n),v)\leq f_n(v) \, ,
\end{align*}
where $f_n:\mathcal{E}\rightarrow [0,+\infty)$ is a sequence of function. Let $n_1 < n_2 < n_3 < \cdots$ be a fixed sequence of integers and consider the policy that runs $\pi$ with horizon $n_1$ until round $t =\min\set{n, n_1}$, then runs $\pi$ with horizon $n_2$ until $t = \min\set{n, n_1 + n_2}$, and then restarts again with horizon $n_3$ until $t = \min\set{n, n_1 + n_2 + n_3}$ and so-on. Note that $t$ is the real-time counter and is not reset on each restart. Let $\pi^*$ be the resulting policy. When $n_{l+1} = 2n_l$ , the length of periods when $\pi$ is used double with each phase, hence the name ‘doubling trick’.

\begin{enumerate}
    \item[(a)]  Let $n > 0$ be arbitrary, $l_{\max} = \min\set{l :\sum_{i=1}^l n_i \geq n}$. Prove that for any $\nu \in\mathcal{E},$ the n-horizon regret of $\pi^{dbl}$ on $\nu$ is at most
    \begin{align*}
        R_n(\pi^*,v)\leq \sum_{l=1}^{l_{\max}}f_{n_l}(v)
    \end{align*}
    \begin{proof}
    \begin{align*}
        R_n(\pi^*,v) = \sum_{l=1}^{l_{\max}} R_{n_l}(\pi(n_l),v) \leq \sum_{l=1}^{l_{\max}} \max_{1\leq t\leq n_l} R_t(\pi(n_l),v)\leq f_{n_l}(v) \leq \sum_{l=1}^{l_{\max}}f_{n_l}(v)
    \end{align*}
    \end{proof}
    \item[(b)] Suppose that $f_n(\nu) \le \sqrt{n}$. Show that if $n_l = 2^l -1$, then for any $\nu \in \mathcal{E}$ and horizon $n$ the regret of $\pi^{dbl}$ is at most
    \begin{align*}
        R_n(\pi^*,v)\leq C\sqrt{n} \, ,
    \end{align*}
    where $C > 0$ is a carefully chosen universal constant.
    \begin{proof}
        Since $\sum_{i=1}^{l}n_i = \sum_{i=0}^{l-1}2^i = 2^l-1$, $l_{\max} = \lceil \log(n+1)\rceil$ and $2^{l_{\max}}\leq 2n+2$. Hence
        \begin{align*}
            R_n(\pi^*,v) \leq \sum_{l=1}^{l_{\max}} \sqrt{2}^{l-1}\leq \frac{1}{\sqrt{2}-1} \sqrt{2^{l_{\max}}} = 2(1+\sqrt{2})\sqrt{n}
        \end{align*}
    \end{proof}
    \item[(c)] Suppose that $f_n(v) = g(v) \log(n)$ for some function $g : \mathcal{E} \to [0, \infty)$. What is the regret of $\pi^*$ if $n_l = 2^l -1$? Can you find a better choice of $(n_l)_l$ ?
    \begin{align*}
        R_n(\pi^*,v)\leq g(v)\sum_{l=1}^{l_{\max}}\log(2^{l-1})
        =\frac{\log(2)}{2}g(v)(l_{\max}-1)l_{\max}\leq C g(v)\log^2(n+1)
    \end{align*}
    where $C$ is some universal constant. A better choice is $n_l = 2^{2^{l-1}}$. With this,
    \begin{align*}
        R_n(\pi^*,v)\leq g(v)\sum_{l=1}^{l_{\max}} \log(2^{2^{l-1}})\leq \log(2)g(v)2^{l_{\max}}\leq Cg(v)\log(n)
    \end{align*}
    where $C>0$ is another universal constant.
\end{enumerate}

\noindent\textbf{6.7} ($\varepsilon$-GREEDY) For this exercise assume the rewards are 1 -subgaussian and there are $k \geq 2$ arms. The $\varepsilon$-greedy algorithm depends on a sequence of parameters $\varepsilon_{1}, \varepsilon_{2}, \ldots . .$ First it chooses each arm once and subsequently chooses $A_{t}=\operatorname{argmax}_{i} \hat{\mu}_{i}(t-1)$ with probability $1-\varepsilon_{t}$ and otherwise chooses an arm uniformly at random.
\begin{enumerate}
    \item[(a)] Prove that if $\varepsilon_{t}=\varepsilon>0$, then $\lim _{n \rightarrow \infty} \frac{R_{n}}{n}=\frac{\varepsilon}{k} \sum_{i=1}^{k} \Delta_{i}$.
    \item[(b)] Let $\Delta_{\min }=\min \left\{\Delta_{i}: \Delta_{i}>0\right\}$ and let $\varepsilon_{t}=\min \left\{1, \frac{C k}{t \Delta_{\min }^{2}}\right\}$, where $C>0$ is a sufficiently large universal constant. Prove that there exists a universal $C^{\prime}>0$ such that
    $$
    R_{n} \leq C^{\prime} \sum_{i=1}^{k}\left(\Delta_{i}+\frac{\Delta_{i}}{\Delta_{\min }^{2}} \log \max \left\{e, \frac{n \Delta_{\min }^{2}}{k}\right\}\right)
    $$
\end{enumerate}

\begin{proof}

\begin{enumerate}
    \item [(a)]
        To prove the $\frac {R_n} {n}$ in $\varepsilon$-GREEDY algorithms, we prove it with uniformly choosing arm and select $A_{t}=\operatorname{argmax}_{i} \hat{\mu}_{i}(t-1)$.
    
        \begin{enumerate}
        \item [1] In uniformly choosing arm situation (with $\varepsilon$ probability), the regret is 
        
            $$
            \begin{aligned}
            & \sum_{t=1}^{n}\left(x-x_{a t}\right) \\
            &=\sum_{i=1}^{n} \mathbb {E} \left(x-x_{a t}\right)\\
            &=\sum_{i=1}^{n} \mathbb {E} \Delta_{i} \\
            &=\sum_{t=1}^{n}\frac{\sum_{i=1}^{k} \Delta_{i}}{k}.
            \end{aligned}
            $$
        
            $\lim _{n \rightarrow \infty} \frac{R_{n}}{n}=\frac{\sum_{i=1}^{k} \Delta_{i}}{k}$
    
        \item [2] In selecting $A_{t}=\operatorname{argmax}_{i} \hat{\mu}_{i}(t-1)$ situation, assuming the best arm is arm 1.
        
            We could prove the select arm is always the best arm 1, when $t$ is large enough. Namely, 
        
            Prove $\exists t_n, t > t_n, A_t = 1$
        
            By contradiction, assuming for $\forall t_n, \exists A_t \neq 1$ in $t > t_n$.
        
            Since $A_t \neq 1$ and $A_{t}=\operatorname{argmax}_{i} \hat{\mu}_{i}(t-1)$, $\hat \mu_i > \hat \mu_1$.
            
            According to reward is 1-subgaussian, 
            $$
            \left | \hat \mu_i - \mu_i \right | < \delta_i
            $$
            
            $$
            \left | \hat \mu_1 - \mu_1 \right | < \delta_1
            $$

            Thus
            $$
            \hat \mu_i < \mu_i + \delta_i \\
            $$
            
            $$
            \hat \mu_1 > \mu_1 - \delta_1
            $$
        
            Let $t_n$ big enough to enable $\delta_i + \delta_1 < \mu_i - \mu_1$.
        
            $\mu_i + \delta_i < \mu_1 - \delta_1$
        
            $\hat \mu_i < \hat \mu_1$ and   $A_t = 1$ which is contradict with our assumption. Thus $\exists t_n, t > t_n, A_t = 1$.
        
            When $t > t_n, A_1 = 1$, $R_t - R_{t_n} = 0$;
        
            % When $t \t_n, R_{t} = \sum_{i = 1} ^ {t_n} {x - x_{a i}} \leq t_n \Delta$
            
            $R_n = R_{tn} + {R_t - R_n} \leq t_n \Delta$
        
            $$
            \lim _{n \rightarrow \infty} \frac{R_{n}}{n}=0
            $$
        \end{enumerate}
        
        The algorithm uniformly chooses arm with $\varepsilon$ probability and selects $A_{t}=\operatorname{argmax}_{i} \hat{\mu}_{i}(t-1)$ with $1 - \varepsilon$ probability.
    
        $\lim _{n \rightarrow \infty} \frac{R_{n}}{n}={\varepsilon}  \frac{\sum_{i=1}^{k} \Delta_{i}} {k} + \left(1 - \varepsilon\right) \cdot 0 = \frac{\varepsilon}{k} \sum_{i=1}^{k} \Delta_{i}$
    \item [(b)]
\end{enumerate}
\end{proof}



\noindent\textbf{6.8} (\textsc{Elimination algorithm})
A simple way to generalise the ETC policy to multiple arms and overcome the problem of tuning the commitment time is to use an elimination algorithm.
The algorithm operates in phases and maintains an active set of arms that could be optimal.
In the $\ell$-th phase, the algorithm aims to eliminate from the active set all arms $i$ for which $\Delta_{i} \geq 2^{-\ell}$.

Without loss of generality, assume that arm 1 is an optimal arm.
You may assume that the horizon $n$ is known.

\begin{enumerate}
    \item[(a)] Show that for any $\ell \geq 1$,
    $$\mathbb{P}\left(1 \notin A_{\ell+1}, 1 \in A_{\ell}\right) \leq k \exp \left(-\frac{m_{\ell} 2^{-2 \ell}}{4}\right).$$

    \item[(b)] Show that if $i \in[k]$ and $\ell \geq 1$ are such that $\Delta_{i} \geq 2^{-\ell}$, then
    $$\mathbb{P}\left(i \in A_{\ell+1}, 1 \in A_{\ell}, i \in A_{\ell}\right) \leq \exp \left(-\frac{m_{\ell}\left(\Delta_{i}-2^{-\ell}\right)^{2}}{4}\right).$$ 
    
    \item[(c)] Let $\ell_{i}=\min \left\{\ell \geq 1: 2^{-\ell} \leq \Delta_{i} / 2\right\}$.
    Choose $m_\ell$ in such a way that $\mathbb{P}\left(\text { exists } \ell: 1 \notin A_{\ell}\right) \leq 1 / n$ and $\mathbb{P}\left(i \in A_{\ell_{i}+1}\right) \leq 1 / n$.

    \item[(d)] Show that your algorithm has regret at most
    $$R_{n} \leq C \sum_{i: \Delta_{i}>0}\left(\Delta_{i}+\frac{1}{\Delta_{i}} \log (n)\right),$$
    where $C>0$ is a carefully chosen universal constant. 
\end{enumerate}

\begin{proof}
    \begin{enumerate}
        \item[(a)] By applying concentration for subgaussian random variables, we have:
        \begin{equation*}
            \begin{aligned}
                \mathbb{P}\left(1 \notin A_{\ell+1}, 1 \in A_{\ell}\right) & \leq \mathbb{P}\left(1 \in A_{\ell}, \text { exists } i \in A_{\ell} \backslash\{1\}: \hat{\mu}_{i, \ell} \geq \hat{\mu}_{1, \ell}+2^{-\ell}\right) \\
                &=\mathbb{P}\left(1 \in A_{\ell}, \text { exists } i \in A_{\ell} \backslash\{1\}: \hat{\mu}_{i, \ell}-\hat{\mu}_{1, \ell} \geq 2^{-\ell}\right) \\
                & \leq k \exp \left(-\frac{m_{\ell} 2^{-2 \ell}}{4}\right),
            \end{aligned}
        \end{equation*} 
        where in the last final inequality we used (c) of Lemma 5.4 and Theorem 5.3.

        \item[(b)] Again, concentration gives that:
        \begin{equation*}
            \begin{aligned}
                \mathbb{P}\left(i \in A_{\ell+1}, 1 \in A_{\ell}, i \in A_{\ell}\right) &\leq \mathbb{P}\left(1 \in A_{\ell}, i \in A_{\ell}, \hat{\mu}_{i, \ell}+2^{-\ell} \geq \hat{\mu}_{1, \ell}\right) \\
                &=\mathbb{P}\left(1 \in A_{\ell}, i \in A_{\ell},\left(\hat{\mu}_{i, \ell}-\mu_{i}\right)-\left(\hat{\mu}_{1, \ell}-\mu_{1}\right) \geq \Delta_{i}-2^{-\ell}\right) \\
                & \leq \exp \left(-\frac{m_{\ell}\left(\Delta_{i}-2^{-\ell}\right)^{2}}{4}\right).
            \end{aligned}
        \end{equation*}

        \item[(c)] We first define
        $$m_{\ell}=2^{4+2 \ell} \log (\ell / \delta),$$
        where $\delta \in(0,1)$ is some constant to be chosen late.
        Then by Part (a),
        \begin{equation*}
            \begin{aligned}
                \mathbb{P}\left(\text { exists } \ell: 1 \notin A_{\ell}\right) & \leq \sum_{\ell=1}^{\infty} \mathbb{P}\left(1 \notin A_{\ell+1}, 1 \in A_{\ell}\right) \\
                & \leq k \sum_{\ell=1}^{\infty} \exp \left(-\frac{m_{\ell} 2^{2 \ell}}{4}\right) \\
                & \leq k \delta \sum_{\ell=1}^{\infty} \frac{1}{\ell^{2}} \\
                &=\frac{k \pi^{2} \delta}{6}.
            \end{aligned}
        \end{equation*}

        Furthermore, by Part (b),
        \begin{equation*}
            \begin{aligned}
                \mathbb{P}\left(i \in A_{\ell_{i}+1}\right) & \leq \mathbb{P}\left(i \in A_{\ell_{i}+1}, i \in A_{\ell_{i}}, 1 \in A_{\ell_{i}}\right)+\mathbb{P}\left(1 \notin A_{\ell_{i}}\right) \\
                & \leq \exp \left(-\frac{m_{\ell}\left(\Delta_{i}-2^{-\ell_{i}}\right)^{2}}{4}\right)+\frac{k \pi^{2} \delta}{6} \\
                & \leq \exp \left(-\frac{m_{\ell} 2^{-2 \ell_{i}}}{16}\right)+\frac{k \pi^{2} \delta}{6} \\
                & \leq \delta\left(1+\frac{k \pi^{2}}{6}\right).
            \end{aligned}
        \end{equation*}

        Choosing $\delta=n^{-1}\left(1+k \pi^{2} / 6\right)^{-1}$ completes the result.
    
        \item[(d)] For $n<k$, all actions need to be tried at most once leading to a trivial result.
        For $n \geq k$, let $i$ be a suboptimal action.
        Notice that $2^{-\ell_{i}} \geq \Delta_{i} / 4$ and hence $2^{2 \ell_{i}} \leq 16 / \Delta_{i}^{2}$.
        Furthermore, $m_{\ell} \geq m_{1} \geq 1$ for $\ell \geq 1$.
        Therefore, we have
        \begin{equation*}
            \begin{aligned}
                \mathbb{E}\left[T_{i}(n)\right] & \leq n \mathbb{P}\left(i \in A_{\ell_{i}+1}\right)+\sum_{\ell=1}^{\ell_{i} \wedge n} m_{\ell} \\
                & \leq 1+\sum_{\ell=1}^{\ell_{i} \wedge n} 2^{4+2 \ell} \log \left(\frac{n}{\delta}\right) \\
                & \leq 1+C 2^{2 \ell_{i}} \log (n k) \\
                & \leq 1+\frac{16 C}{\Delta_{i}^{2}} \log (n k)
            \end{aligned}
        \end{equation*}
        where $C>1$ is a suitably large universal constant derived by naively bounding the logarithmic
        term and the geometric series.
        The result follows from upper bounding $\log (n k) \leq 2 \log (n)$ which follows from $k \leq n$ and the standard regret decomposition, as described in Lemma 4.5. 

    \end{enumerate}
\end{proof}